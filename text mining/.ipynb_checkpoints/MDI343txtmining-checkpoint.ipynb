{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP MDI343: Application à la classification : l’analyse d’opinions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation du classifieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk import pos_tag\n",
    "from sklearn.svm import LinearSVC\n",
    "from glob import glob\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk import SnowballStemmer\n",
    "import nltk\n",
    "ipython nbconvert --to pdf notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "2000 documents\n"
     ]
    }
   ],
   "source": [
    "# Chargement des textes de critiques\n",
    "print(\"Loading dataset\")\n",
    "\n",
    "from glob import glob\n",
    "filenames_neg = sorted(glob(op.join('.', 'data', 'imdb1', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(op.join('.', 'data', 'imdb1', 'pos', '*.txt')))\n",
    "\n",
    "texts_neg = [open(f).read() for f in filenames_neg]\n",
    "texts_pos = [open(f).read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "\n",
    "print(\"%d documents\" % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Compléter la fonction count_words qui va compter le nombre d’occurrences de chaque mot dans une liste de string et renvoyer le vocabulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Méthode countword:\n",
    "\n",
    "def count_words(texts):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "        n_samples == number of documents.\n",
    "        n_features == number of words in vocabulary.\n",
    "    \"\"\"\n",
    "    words = set()\n",
    "    vocabulary = {}\n",
    "    list_voc1 = []\n",
    "    \n",
    "    for text in texts:   \n",
    "        voc1 = {}\n",
    "        for word in text.split(\" \"):\n",
    "            if word not in vocabulary :\n",
    "                vocabulary[word] = 1\n",
    "                if word not in voc1:\n",
    "                    voc1[word] = 1\n",
    "                else:\n",
    "                    voc1[word] += 1\n",
    "            else:\n",
    "                vocabulary[word] += 1\n",
    "                if word not in voc1:\n",
    "                    voc1[word] = 1\n",
    "                else:\n",
    "                    voc1[word] += 1\n",
    "                \n",
    "        list_voc1.append(voc1)\n",
    "        #pass\n",
    "    counts = pd.DataFrame(list_voc1).fillna(0)\n",
    "     \n",
    "    return vocabulary, counts, list_voc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countword = count_words(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode retourne \n",
    "* le vocabulaire présent dans les textes en format dictionnaire\n",
    "* Une table avec le nombre d'occurence des mots du vocabulary dans chaque textes en format Dataframe, avec en colonne les mots et en ligne les textes. (le format permet ainsi de faire de la recherche spécifique sur un texte).\n",
    "* La liste des vocabulaires dans chaque texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Expliquer comment les classes positives et négatives ont été assignées sur les critiques de films (voir fichier poldata.README.2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a set of ad-hoc rule:\n",
    "\n",
    "* numerical ratings and star ratings.  (\"8/10\", \"four out of five\", and \"OUT OF ****: ***\" are examples of rating indications we recognize.)\n",
    "\n",
    "* five star/four star/letter grade notation stystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Compléter la classe NB pour qu’elle implémente le classifieur Naive Bayes en vous appuyant sur le pseudo-code de la figure 1 et sa documentation ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    #def __init__(self,Vocabulary,countDocs):\n",
    "        #self.Voc = Vocabulary\n",
    "        #self.countDoc = countDocs\n",
    "        \n",
    "\n",
    "    def fit(self,X, y):\n",
    "        N=len(X)\n",
    "        A = X.copy()\n",
    "        A['class'] = y\n",
    "        B = A.groupby(['class']).sum()\n",
    "            \n",
    "        self.prob_pos = (B.iloc[0]+1)/(B.iloc[0]+1).sum()\n",
    "        self.prob_neg = (B.iloc[1]+1)/(B.iloc[1]+1).sum()\n",
    "        self.prior_neg = sum(y)/N\n",
    "        self.prior_pos = (N - sum(y))/N\n",
    "        return (self)\n",
    "\n",
    "    \n",
    "    def predict(self,X): \n",
    "        self.classpred = np.zeros(len(X))\n",
    "        scorec_pos = np.zeros(len(X))\n",
    "        scorec_neg = np.zeros(len(X))       \n",
    "        for i in range (len(X)):\n",
    "            scorec_pos[i] = log(self.prior_pos)\n",
    "            scorec_neg[i] = log(self.prior_neg)\n",
    "            for word in X.columns[X.iloc[i,:] !=0]:\n",
    "                if word in self.prob_pos.index:\n",
    "                    scorec_pos[i] += log(self.prob_pos[word])\n",
    "                    scorec_neg[i] += log(self.prob_neg[word])\n",
    "                else:\n",
    "                    pass\n",
    "            self.classpred[i] = np.array(np.argmax([scorec_pos[i],scorec_neg[i]]))\n",
    "        return (self.classpred)\n",
    "   \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82699999999999996"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y =list(np.repeat(1,len(texts_neg))) + list(np.repeat(0,len(texts_pos)))\n",
    "X = countword[1]\n",
    "\n",
    "Xtrain = X[::2]\n",
    "ytrain = y[::2]\n",
    "\n",
    "Xtest = X[1::2]\n",
    "ytest = y[1::2]\n",
    "\n",
    "NaivBay = NB()\n",
    "NaivBay.fit(Xtrain,ytrain)\n",
    "print('Le score de prédiction est : ' ,NaivBay.score(Xtest,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Evaluer les performances de votre classifieur en cross-validation 5-folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.80389222,  0.81081081,  0.83033033])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Les scores de CV sont : 'cross_val_score(NaivBay, X.drop('fit',axis=1), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Modifiez la fonction count_words pour qu’elle ignore les “stop words” dans le fichier data/english.stop. Les performances sont-elles améliorées?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# On charge les stops words english fournis.\n",
    "stop = open('./data/english.stop').read().split()\n",
    "\n",
    "# On rajoute à cette liste la ponctuation.\n",
    "ponctuation = ['.','\"',\")\",\"(\",\" \",\";\",'\\'','  ','!',':','/']\n",
    "stop.extend(ponctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Méthode countwords en retirant les stops words.\n",
    "\n",
    "def count_words_stop(texts,stopwords):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "        n_samples == number of documents.\n",
    "        n_features == number of words in vocabulary.\n",
    "    \"\"\"\n",
    "    words = set()\n",
    "    vocabulary = {}\n",
    "    list_voc1 = []\n",
    "    \n",
    "    for text in texts:   \n",
    "        voc1 = {}\n",
    "        for word in text.split():\n",
    "            if word not in stopwords:\n",
    "                if word not in vocabulary :\n",
    "                    vocabulary[word] = 1\n",
    "                    if word not in voc1:\n",
    "                        voc1[word] = 1\n",
    "                    else:\n",
    "                        voc1[word] += 1\n",
    "                   \n",
    "                else:\n",
    "                    vocabulary[word] += 1\n",
    "                    if word not in voc1:\n",
    "                        voc1[word] = 1\n",
    "                    else:\n",
    "                        voc1[word] += 1\n",
    "            else:\n",
    "                pass\n",
    "        list_voc1.append(voc1)\n",
    "        #pass\n",
    "    counts = pd.DataFrame(list_voc1).fillna(0)\n",
    "     \n",
    "    return vocabulary, counts, list_voc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.80239521  0.83033033  0.82582583]\n"
     ]
    }
   ],
   "source": [
    "wordcount_stop = count_words_stop(texts,stop)\n",
    "X = wordcount_stop[1]\n",
    "NaivBay_stop = NB()\n",
    "print('Les scores de CV sont : 'cross_val_score(NaivBay_stop, X.drop('fit',axis=1), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le meilleur score de CV n'est pas amélioré en enlevant les stops words.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation de scikitlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 : Comparer votre implémentation avec scikitlearn. On utilisera la classe CountVectorizer et un Pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleur score :   0.791\n",
      "Paramètres retenus :   {'CV__analyzer': 'word', 'CV__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "CV = CountVectorizer(stop_words = stop)\n",
    "MNB = MultinomialNB()\n",
    "pipeline = Pipeline([('CV', CV), ('MNB', MNB)])\n",
    "\n",
    "parameters = {'CV__analyzer' : ['char', 'word','char_wb'],'CV__ngram_range':[(1,1),(2,2)]}\n",
    "\n",
    "GSCV = GridSearchCV(pipeline, parameters,cv=3, return_train_score=False)\n",
    "\n",
    "\n",
    "GSCV.fit(texts,y)\n",
    "results = pd.DataFrame(GSCV.cv_results_)\n",
    "\n",
    "print('Meilleur score :  ', GSCV.best_score_)\n",
    "print('Paramètres retenus :  ', GSCV.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 : Tester un autre algorithme de la librairie scikitlearn (ex : LinearSVC, LogisticRegression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CV = CountVectorizer(stop_words = stop)\n",
    "LSVC = LinearSVC()\n",
    "pipeline = Pipeline([('CV', CV), ('LSVC', LSVC)])\n",
    "\n",
    "parameters = {'CV__analyzer' : ['char', 'word','char_wb'],'CV__ngram_range':[(1,1),(1,2),(2,2)]}\n",
    "\n",
    "GSCV = GridSearchCV(pipeline, parameters,cv=3, return_train_score=False)\n",
    "\n",
    "\n",
    "GSCV.fit(texts,y)\n",
    "results = pd.DataFrame(GSCV.cv_results_)\n",
    "\n",
    "print('Meilleur score :  ', GSCV.best_score_)\n",
    "print('Paramètres retenus :  ', GSCV.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 : Utiliser la librairie NLTK afin de procéder à une racinisation (stemming). Vous utiliserez la classe SnowballStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_stemmed =[\" \".join(stemmer.stem(word) for word in text.split()) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CV = CountVectorizer(stop_words = stop)\n",
    "LSVC = LinearSVC()\n",
    "pipeline = Pipeline([('CV', CV), ('LSVC', LSVC)])\n",
    "\n",
    "parameters = {'CV__analyzer' : ['char', 'word','char_wb'],'CV__ngram_range':[(1,1),(1,2),(2,2)]}\n",
    "\n",
    "GSCV = GridSearchCV(pipeline, parameters,cv=3, return_train_score=False)\n",
    "\n",
    "\n",
    "GSCV.fit(texts_stemmed,y)\n",
    "results = pd.DataFrame(GSCV.cv_results_)\n",
    "\n",
    "print('Meilleur score :  ', GSCV.best_score_)\n",
    "print('Paramètres retenus :  ', GSCV.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 : Filtrer les mots par catégorie grammaticale (POS : Part Of Speech) et ne garder que les noms, les verbes, les adverbes et les adjectifs pour la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(\".VB.|NN|JJ|.RB\")\n",
    "texts_pos = [\" \".join(tuple[0] if pattern.match(tuple[1]) else '' \\\n",
    "                          for tuple in nltk.pos_tag(text.split())) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CV = CountVectorizer(stop_words = stop)\n",
    "LSVC = LinearSVC()\n",
    "pipeline = Pipeline([('CV', CV), ('LSVC', LSVC)])\n",
    "\n",
    "parameters = {'CV__analyzer' : ['char', 'word','char_wb'],'CV__ngram_range':[(1,1)]}\n",
    "\n",
    "GSCV = GridSearchCV(pipeline, parameters,cv=3, return_train_score=False)\n",
    "\n",
    "\n",
    "GSCV.fit(texts_pos,y)\n",
    "results = pd.DataFrame(GSCV.cv_results_)\n",
    "\n",
    "print('Meilleur score :  ', GSCV.best_score_)\n",
    "print('Paramètres retenus :  ', GSCV.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
